{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wScw6C71nB3U"
      },
      "source": [
        "Sunspot Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56XEQOGknrAk",
        "outputId": "89dec066-d6a5-4575-e7a2-b2c376383653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# plot command\n",
        "\n",
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcG9r1eClbTh",
        "outputId": "89187bcf-a925-4ed3-d5be-98f2dcb6f094",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv \\\n",
        "    -O /tmp/sunspots.csv\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "time_step = []\n",
        "sunspots = []\n",
        "\n",
        "with open('/tmp/sunspots.csv') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "    sunspots.append(float(row[2]))\n",
        "    time_step.append(int(row[0]))\n",
        "\n",
        "series = np.array(sunspots)\n",
        "time = np.array(time_step)\n",
        "#plt.figure(figsize=(10, 6))\n",
        "#plot_series(time, series)\n",
        "\n",
        "\n",
        "# split_time is setting up the train / split\n",
        "\n",
        "split_time = 3000\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]\n",
        "\n",
        "window_size = 30\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-11 00:03:53--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.9.208, 172.217.164.176, 172.253.122.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.9.208|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70827 (69K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/sunspots.csv’\n",
            "\n",
            "\r/tmp/sunspots.csv     0%[                    ]       0  --.-KB/s               \r/tmp/sunspots.csv   100%[===================>]  69.17K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2020-11-11 00:03:53 (121 MB/s) - ‘/tmp/sunspots.csv’ saved [70827/70827]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XwGrf-A_wF0"
      },
      "source": [
        "def model_forecast(model, series, window_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(32).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JviH9kKkjP_h"
      },
      "source": [
        "### Format Windowed Data & Build Dense Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOXOZbkwXnFr"
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "                                    \n",
        "    tf.keras.layers.Dense(20,input_shape=[window_size], activation='relu'),\n",
        "    tf.keras.layers.Dense(10,activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-7,momentum=0.9))\n",
        "history = model.fit(dataset,epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9IB5mCoQxbr"
      },
      "source": [
        "### Creating a Time Series dataset Windowing & Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pxZFlTc2WVf",
        "outputId": "c4f1b8dc-70af-4e9d-aed8-b9ee992ae36c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# shuffling prevents the net from getting stuck in local minima\n",
        "\n",
        "window_size = 30\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "# put into tf format\n",
        "temp = tf.data.Dataset.from_tensor_slices(series)\n",
        "# this just sets up the dimension of the window\n",
        "# want 30+1 because eventually that 1 will be split into the response \n",
        "#   for the previous 30\n",
        "# shift by 1 because we are using 30 previous points to predict the next one\n",
        "temp = temp.window(window_size+1,shift=1,drop_remainder=True)\n",
        "\n",
        "\n",
        "# each set of 31 here is:\n",
        "#   1st: first 31 values\n",
        "#   2nd: shifted forward by one value\n",
        "temp = temp.flat_map(lambda w: w.batch(window_size+1))\n",
        "temp = temp.shuffle(shuffle_buffer_size)\n",
        "\n",
        "\n",
        "# this takes each batch & turns it into a tuple\n",
        "# the training points are the first 30\n",
        "# the response is the last number\n",
        "temp = temp.map(lambda w: (w[:-1], w[:1:]))\n",
        "\n",
        "\n",
        "# this turns the data into a tuple of 32 batches of 30\n",
        "# along with 30 responses\n",
        "# prefetch is a performance thing\n",
        "temp = temp.batch(batch_size).prefetch(1)\n",
        "temp\n",
        "\n",
        "\n",
        "#def sub_to_batch(sub):\n",
        "#  return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "#for i in temp:\n",
        "#  for j in i:\n",
        "#    print(j.numpy())\n",
        "\n",
        "#for example in temp.take(5):\n",
        "#  print(example.numpy())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, None), (None, None)), types: (tf.float64, tf.float64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMmmIHNojXQ7"
      },
      "source": [
        "### Conv1D + LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZoufAKqio9s",
        "outputId": "7860d542-0f2b-45e1-a8f5-3dabe7ead7a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "    \n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "\n",
        "window_size = 30\n",
        "batch_size = 32\n",
        "train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "print(train_set)\n",
        "print(x_train.shape)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n",
        "                      strides=1, padding=\"causal\",\n",
        "                      activation=\"relu\",\n",
        "                      input_shape=[None, 1]),\n",
        "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  # since our predictions are in the 1 to 400 range\n",
        "  tf.keras.layers.Lambda(lambda x: x * 400)\n",
        "])\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "model.summary()\n",
        "history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float64, tf.float64)>\n",
            "(3000,)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, None, 32)          192       \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 64)          24832     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 64)          33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 30)          1950      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 10)          310       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, None, 1)           11        \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, None, 1)           0         \n",
            "=================================================================\n",
            "Total params: 60,319\n",
            "Trainable params: 60,319\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 66.7489 - mae: 67.2457\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 54.7390 - mae: 55.2354\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 47.3742 - mae: 47.8704\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 43.3387 - mae: 43.8349\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 41.0206 - mae: 41.5173\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 39.4359 - mae: 39.9325\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 38.1357 - mae: 38.6322\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 37.0225 - mae: 37.5190\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 35.9965 - mae: 36.4930\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 35.0764 - mae: 35.5724\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 34.2563 - mae: 34.7525\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 33.5417 - mae: 34.0377\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 32.8471 - mae: 33.3431\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 32.1882 - mae: 32.6840\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 31.6020 - mae: 32.0977\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 30.9888 - mae: 31.4843\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 30.5427 - mae: 31.0381\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 29.9659 - mae: 30.4613\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 29.4553 - mae: 29.9505\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 28.9844 - mae: 29.4798\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 28.5881 - mae: 29.0833\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 28.1728 - mae: 28.6680\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 27.7892 - mae: 28.2845\n",
            "Epoch 24/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 27.6933 - mae: 28.1883\n",
            "Epoch 25/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 27.0644 - mae: 27.5592\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 26.7731 - mae: 27.2680\n",
            "Epoch 27/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 26.4067 - mae: 26.9014\n",
            "Epoch 28/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 25.9881 - mae: 26.4829\n",
            "Epoch 29/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 25.6091 - mae: 26.1040\n",
            "Epoch 30/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 25.3655 - mae: 25.8600\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 24.8784 - mae: 25.3728\n",
            "Epoch 32/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 24.5847 - mae: 25.0792\n",
            "Epoch 33/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 24.1643 - mae: 24.6586\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 23.7227 - mae: 24.2170\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 23.2968 - mae: 23.7911\n",
            "Epoch 36/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 22.9594 - mae: 23.4536\n",
            "Epoch 37/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 22.5600 - mae: 23.0545\n",
            "Epoch 38/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 22.0825 - mae: 22.5764\n",
            "Epoch 39/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 21.7523 - mae: 22.2460\n",
            "Epoch 40/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 21.4995 - mae: 21.9935\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 21.2081 - mae: 21.7020\n",
            "Epoch 42/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 20.7664 - mae: 21.2600\n",
            "Epoch 43/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 20.5361 - mae: 21.0295\n",
            "Epoch 44/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 20.3581 - mae: 20.8512\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 20.0736 - mae: 20.5667\n",
            "Epoch 46/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 19.7762 - mae: 20.2691\n",
            "Epoch 47/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 19.4989 - mae: 19.9915\n",
            "Epoch 48/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 19.3252 - mae: 19.8179\n",
            "Epoch 49/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 19.1726 - mae: 19.6652\n",
            "Epoch 50/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.9619 - mae: 19.4541\n",
            "Epoch 51/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 19.0075 - mae: 19.4999\n",
            "Epoch 52/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 18.7514 - mae: 19.2438\n",
            "Epoch 53/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.6668 - mae: 19.1593\n",
            "Epoch 54/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.4004 - mae: 18.8922\n",
            "Epoch 55/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.7272 - mae: 19.2193\n",
            "Epoch 56/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 18.4924 - mae: 18.9843\n",
            "Epoch 57/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.4745 - mae: 18.9663\n",
            "Epoch 58/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.2727 - mae: 18.7644\n",
            "Epoch 59/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.1660 - mae: 18.6577\n",
            "Epoch 60/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 17.9287 - mae: 18.4204\n",
            "Epoch 61/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 17.9468 - mae: 18.4385\n",
            "Epoch 62/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.2452 - mae: 18.7368\n",
            "Epoch 63/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.6162 - mae: 19.1084\n",
            "Epoch 64/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 17.9889 - mae: 18.4802\n",
            "Epoch 65/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 18.9835 - mae: 19.4755\n",
            "Epoch 66/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.5703 - mae: 19.0622\n",
            "Epoch 67/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.6349 - mae: 19.1268\n",
            "Epoch 68/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.4438 - mae: 18.9358\n",
            "Epoch 69/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 19.4966 - mae: 19.9891\n",
            "Epoch 70/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 17.9926 - mae: 18.4844\n",
            "Epoch 71/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.0540 - mae: 18.5455\n",
            "Epoch 72/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.0604 - mae: 18.5519\n",
            "Epoch 73/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 18.7476 - mae: 19.2396\n",
            "Epoch 74/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 18.4687 - mae: 18.9609\n",
            "Epoch 75/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.2951 - mae: 18.7868\n",
            "Epoch 76/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 18.3331 - mae: 18.8251\n",
            "Epoch 77/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 20.1684 - mae: 20.6615\n",
            "Epoch 78/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 19.7032 - mae: 20.1960\n",
            "Epoch 79/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 18.6782 - mae: 19.1708\n",
            "Epoch 80/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 21.3245 - mae: 21.8181\n",
            "Epoch 81/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 22.2154 - mae: 22.7090\n",
            "Epoch 82/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 20.6901 - mae: 21.1835\n",
            "Epoch 83/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 18.9178 - mae: 19.4106\n",
            "Epoch 84/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 23.2210 - mae: 23.7150\n",
            "Epoch 85/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 19.1649 - mae: 19.6577\n",
            "Epoch 86/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 21.7781 - mae: 22.2718\n",
            "Epoch 87/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 20.0484 - mae: 20.5413\n",
            "Epoch 88/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 24.2601 - mae: 24.7547\n",
            "Epoch 89/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 31.2376 - mae: 31.7335\n",
            "Epoch 90/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 29.4868 - mae: 29.9826\n",
            "Epoch 91/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 46.1719 - mae: 46.6695\n",
            "Epoch 92/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 57.5550 - mae: 58.0533\n",
            "Epoch 93/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 59.8991 - mae: 60.3972\n",
            "Epoch 94/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 56.7264 - mae: 57.2244\n",
            "Epoch 95/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 57.8160 - mae: 58.3142\n",
            "Epoch 96/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 59.1699 - mae: 59.6683\n",
            "Epoch 97/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 62.2277 - mae: 62.7261\n",
            "Epoch 98/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 61.1453 - mae: 61.6434\n",
            "Epoch 99/100\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 63.5653 - mae: 64.0636\n",
            "Epoch 100/100\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 63.9286 - mae: 64.4268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPkEYQE5j4-S"
      },
      "source": [
        "### Plot Learning Rate & Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NicLJWPio6z",
        "outputId": "6fbac4a9-f5dd-464b-ac6e-1f1c1ef54785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"]) \n",
        "plt.axis([1e-8, 1e-4, 0, 60])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1e-08, 0.0001, 0.0, 60.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwBRETA8kEA1"
      },
      "source": [
        "### Update Model based on learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1cDxGlvkDXJ",
        "outputId": "34f1ccde-9778-474e-a5b8-1155f5df7e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# changing window_size, batch_size, filters, & layer sizes\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "train_set = windowed_dataset(x_train, window_size=30, batch_size=32, shuffle_buffer=1000)\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
        "                      strides=1, padding=\"causal\",\n",
        "                      activation=\"relu\",\n",
        "                      input_shape=[None, 1]),\n",
        "  tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "  tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(1)#,\n",
        "  #tf.keras.layers.Lambda(lambda x: x * 400)\n",
        "])\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "history = model.fit(train_set,epochs=60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 82.4822 - mae: 82.9704\n",
            "Epoch 2/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 82.3575 - mae: 82.8456\n",
            "Epoch 3/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 82.2480 - mae: 82.7366\n",
            "Epoch 4/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 82.1674 - mae: 82.6565\n",
            "Epoch 5/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 82.0957 - mae: 82.5853\n",
            "Epoch 6/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 82.0240 - mae: 82.5142\n",
            "Epoch 7/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.9511 - mae: 82.4419\n",
            "Epoch 8/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.8755 - mae: 82.3670\n",
            "Epoch 9/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.7913 - mae: 82.2834\n",
            "Epoch 10/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.6998 - mae: 82.1926\n",
            "Epoch 11/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.6055 - mae: 82.0989\n",
            "Epoch 12/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.5059 - mae: 81.9999\n",
            "Epoch 13/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.4015 - mae: 81.8960\n",
            "Epoch 14/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.2902 - mae: 81.7850\n",
            "Epoch 15/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.1692 - mae: 81.6643\n",
            "Epoch 16/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 81.0372 - mae: 81.5327\n",
            "Epoch 17/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 80.8960 - mae: 81.3918\n",
            "Epoch 18/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 80.7458 - mae: 81.2419\n",
            "Epoch 19/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 80.5846 - mae: 81.0807\n",
            "Epoch 20/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 80.4100 - mae: 80.9062\n",
            "Epoch 21/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 80.2200 - mae: 80.7161\n",
            "Epoch 22/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 80.0120 - mae: 80.5081\n",
            "Epoch 23/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 79.7829 - mae: 80.2792\n",
            "Epoch 24/60\n",
            "93/93 [==============================] - 2s 18ms/step - loss: 79.5278 - mae: 80.0243\n",
            "Epoch 25/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 79.2432 - mae: 79.7397\n",
            "Epoch 26/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 78.9314 - mae: 79.4281\n",
            "Epoch 27/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 78.5875 - mae: 79.0846\n",
            "Epoch 28/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 78.2039 - mae: 78.7010\n",
            "Epoch 29/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 77.7733 - mae: 78.2703\n",
            "Epoch 30/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 77.2875 - mae: 77.7844\n",
            "Epoch 31/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 76.7376 - mae: 77.2347\n",
            "Epoch 32/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 76.1145 - mae: 76.6111\n",
            "Epoch 33/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 75.4061 - mae: 75.9030\n",
            "Epoch 34/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 74.6009 - mae: 75.0976\n",
            "Epoch 35/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 73.6943 - mae: 74.1909\n",
            "Epoch 36/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 72.6759 - mae: 73.1722\n",
            "Epoch 37/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 71.5387 - mae: 72.0350\n",
            "Epoch 38/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 70.2859 - mae: 70.7824\n",
            "Epoch 39/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 68.9102 - mae: 69.4064\n",
            "Epoch 40/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 67.4277 - mae: 67.9243\n",
            "Epoch 41/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 65.8635 - mae: 66.3606\n",
            "Epoch 42/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 64.2234 - mae: 64.7210\n",
            "Epoch 43/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 62.4905 - mae: 62.9881\n",
            "Epoch 44/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 60.6909 - mae: 61.1882\n",
            "Epoch 45/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 58.8654 - mae: 59.3629\n",
            "Epoch 46/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 56.8470 - mae: 57.3445\n",
            "Epoch 47/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 54.3550 - mae: 54.8521\n",
            "Epoch 48/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 51.2362 - mae: 51.7329\n",
            "Epoch 49/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 47.1355 - mae: 47.6317\n",
            "Epoch 50/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 42.9267 - mae: 43.4222\n",
            "Epoch 51/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 39.2605 - mae: 39.7551\n",
            "Epoch 52/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 36.5027 - mae: 36.9972\n",
            "Epoch 53/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 34.5339 - mae: 35.0284\n",
            "Epoch 54/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 32.3724 - mae: 32.8668\n",
            "Epoch 55/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 30.5127 - mae: 31.0068\n",
            "Epoch 56/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 29.3491 - mae: 29.8430\n",
            "Epoch 57/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 27.6702 - mae: 28.1640\n",
            "Epoch 58/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 27.0529 - mae: 27.5465\n",
            "Epoch 59/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 26.1410 - mae: 26.6345\n",
            "Epoch 60/60\n",
            "93/93 [==============================] - 2s 19ms/step - loss: 25.2278 - mae: 25.7213\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}