{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TabNet Work.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f60-fnpKZQ4W"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7H-PMXvnN2T"
      },
      "source": [
        "# TabNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSDXuynmnB73"
      },
      "source": [
        "def glu(act, n_units):\n",
        "  \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
        "  return act[:, :n_units] * tf.nn.sigmoid(act[:, n_units:])\n",
        "\n",
        "\n",
        "class TabNet(object):\n",
        "  \"\"\"TabNet model class.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               columns,\n",
        "               num_features,\n",
        "               feature_dim,\n",
        "               output_dim,\n",
        "               num_decision_steps,\n",
        "               relaxation_factor,\n",
        "               batch_momentum,\n",
        "               virtual_batch_size,\n",
        "               num_classes,\n",
        "               epsilon=0.00001):\n",
        "\n",
        "    self.columns = columns\n",
        "    self.num_features = num_features\n",
        "    self.feature_dim = feature_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_decision_steps = num_decision_steps\n",
        "    self.relaxation_factor = relaxation_factor\n",
        "    self.batch_momentum = batch_momentum\n",
        "    self.virtual_batch_size = virtual_batch_size\n",
        "    self.num_classes = num_classes\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def encoder(self, data, reuse, is_training):\n",
        "    \"\"\"TabNet encoder model.\"\"\"\n",
        "\n",
        "    with tf.variable_scope(\"Encoder\", reuse=reuse):\n",
        "\n",
        "      # Reads and normalizes input features.\n",
        "      features = tf.feature_column.input_layer(data, self.columns)\n",
        "      features = tf.layers.batch_normalization(\n",
        "          features, training=is_training, momentum=self.batch_momentum)\n",
        "      batch_size = tf.shape(features)[0]\n",
        "\n",
        "      # Initializes decision-step dependent variables.\n",
        "      output_aggregated = tf.zeros([batch_size, self.output_dim])\n",
        "      masked_features = features\n",
        "      mask_values = tf.zeros([batch_size, self.num_features])\n",
        "      aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n",
        "      complemantary_aggregated_mask_values = tf.ones(\n",
        "          [batch_size, self.num_features])\n",
        "      total_entropy = 0\n",
        "\n",
        "      if is_training:\n",
        "        v_b = self.virtual_batch_size\n",
        "      else:\n",
        "        v_b = 1\n",
        "\n",
        "      for ni in range(self.num_decision_steps):\n",
        "\n",
        "        # Feature transformer with two shared and two decision step dependent\n",
        "        # blocks is used below.\n",
        "\n",
        "        reuse_flag = (ni > 0)\n",
        "\n",
        "        transform_f1 = tf.layers.dense(\n",
        "            masked_features,\n",
        "            self.feature_dim * 2,\n",
        "            name=\"Transform_f1\",\n",
        "            reuse=reuse_flag,\n",
        "            use_bias=False)\n",
        "        transform_f1 = tf.layers.batch_normalization(\n",
        "            transform_f1,\n",
        "            training=is_training,\n",
        "            momentum=self.batch_momentum,\n",
        "            virtual_batch_size=v_b)\n",
        "        transform_f1 = glu(transform_f1, self.feature_dim)\n",
        "\n",
        "        transform_f2 = tf.layers.dense(\n",
        "            transform_f1,\n",
        "            self.feature_dim * 2,\n",
        "            name=\"Transform_f2\",\n",
        "            reuse=reuse_flag,\n",
        "            use_bias=False)\n",
        "        transform_f2 = tf.layers.batch_normalization(\n",
        "            transform_f2,\n",
        "            training=is_training,\n",
        "            momentum=self.batch_momentum,\n",
        "            virtual_batch_size=v_b)\n",
        "        transform_f2 = (glu(transform_f2, self.feature_dim) +\n",
        "                        transform_f1) * np.sqrt(0.5)\n",
        "\n",
        "        transform_f3 = tf.layers.dense(\n",
        "            transform_f2,\n",
        "            self.feature_dim * 2,\n",
        "            name=\"Transform_f3\" + str(ni),\n",
        "            use_bias=False)\n",
        "        transform_f3 = tf.layers.batch_normalization(\n",
        "            transform_f3,\n",
        "            training=is_training,\n",
        "            momentum=self.batch_momentum,\n",
        "            virtual_batch_size=v_b)\n",
        "        transform_f3 = (glu(transform_f3, self.feature_dim) +\n",
        "                        transform_f2) * np.sqrt(0.5)\n",
        "\n",
        "        transform_f4 = tf.layers.dense(\n",
        "            transform_f3,\n",
        "            self.feature_dim * 2,\n",
        "            name=\"Transform_f4\" + str(ni),\n",
        "            use_bias=False)\n",
        "        transform_f4 = tf.layers.batch_normalization(\n",
        "            transform_f4,\n",
        "            training=is_training,\n",
        "            momentum=self.batch_momentum,\n",
        "            virtual_batch_size=v_b)\n",
        "        transform_f4 = (glu(transform_f4, self.feature_dim) +\n",
        "                        transform_f3) * np.sqrt(0.5)\n",
        "\n",
        "        if ni > 0:\n",
        "\n",
        "          decision_out = tf.nn.relu(transform_f4[:, :self.output_dim])\n",
        "\n",
        "          # Decision aggregation.\n",
        "          output_aggregated += decision_out\n",
        "\n",
        "          # Aggregated masks are used for visualization of the\n",
        "          # feature importance attributes.\n",
        "          scale_agg = tf.reduce_sum(\n",
        "              decision_out, axis=1, keep_dims=True) / (\n",
        "                  self.num_decision_steps - 1)\n",
        "          aggregated_mask_values += mask_values * scale_agg\n",
        "\n",
        "        features_for_coef = (transform_f4[:, self.output_dim:])\n",
        "\n",
        "        if ni < self.num_decision_steps - 1:\n",
        "\n",
        "          # Determines the feature masks via linear and nonlinear\n",
        "          # transformations, taking into account of aggregated feature use.\n",
        "          mask_values = tf.layers.dense(\n",
        "              features_for_coef,\n",
        "              self.num_features,\n",
        "              name=\"Transform_coef\" + str(ni),\n",
        "              use_bias=False)\n",
        "          mask_values = tf.layers.batch_normalization(\n",
        "              mask_values,\n",
        "              training=is_training,\n",
        "              momentum=self.batch_momentum,\n",
        "              virtual_batch_size=v_b)\n",
        "          mask_values *= complemantary_aggregated_mask_values\n",
        "          mask_values = tf.contrib.sparsemax.sparsemax(mask_values)\n",
        "\n",
        "          # Relaxation factor controls the amount of reuse of features between\n",
        "          # different decision blocks and updated with the values of\n",
        "          # coefficients.\n",
        "          complemantary_aggregated_mask_values *= (\n",
        "              self.relaxation_factor - mask_values)\n",
        "\n",
        "          # Entropy is used to penalize the amount of sparsity in feature\n",
        "          # selection.\n",
        "          total_entropy += tf.reduce_mean(\n",
        "              tf.reduce_sum(\n",
        "                  -mask_values * tf.log(mask_values + self.epsilon),\n",
        "                  axis=1)) / (\n",
        "                      self.num_decision_steps - 1)\n",
        "\n",
        "          # Feature selection.\n",
        "          masked_features = tf.multiply(mask_values, features)\n",
        "\n",
        "          # Visualization of the feature selection mask at decision step ni\n",
        "          tf.summary.image(\n",
        "              \"Mask for step\" + str(ni),\n",
        "              tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n",
        "              max_outputs=1)\n",
        "\n",
        "      # Visualization of the aggregated feature importances\n",
        "      tf.summary.image(\n",
        "          \"Aggregated mask\",\n",
        "          tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n",
        "          max_outputs=1)\n",
        "\n",
        "      return output_aggregated, total_entropy\n",
        "\n",
        "  def classify(self, activations, reuse):\n",
        "    \"\"\"TabNet classify block.\"\"\"\n",
        "\n",
        "    with tf.variable_scope(\"Classify\", reuse=reuse):\n",
        "      logits = tf.layers.dense(activations, self.num_classes, use_bias=False)\n",
        "      predictions = tf.nn.softmax(logits)\n",
        "      return logits, predictions\n",
        "\n",
        "  def regress(self, activations, reuse):\n",
        "    \"\"\"TabNet regress block.\"\"\"\n",
        "\n",
        "    with tf.variable_scope(\"Regress\", reuse=reuse):\n",
        "      predictions = tf.layers.dense(activations, 1)\n",
        "      return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u996rnXktZcy"
      },
      "source": [
        "# To Do\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F83134_nT3H"
      },
      "source": [
        "Application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-6dzMPxnz7_"
      },
      "source": [
        "path = \"https://raw.githubusercontent.com/shrikant-temburwar/Wine-Quality-Dataset/master/winequality-red.csv\"\n",
        "df_r = pd.read_csv(path,delimiter=\";\")\n",
        "#df_r['Color'] = 1\n",
        "\n",
        "path = \"https://raw.githubusercontent.com/shrikant-temburwar/Wine-Quality-Dataset/master/winequality-white.csv\"\n",
        "df_w = pd.read_csv(path,delimiter=\";\")\n",
        "#df_w['Color'] = 0\n",
        "#df = df_r.append(df_w)\n",
        "df = df_w\n",
        "df\n",
        "X = df.loc[:,df.columns != 'quality']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsdebA1vocGT"
      },
      "source": [
        "# Broken Down TabNet\n",
        "\n",
        "# something to note, this loop is being fed ALREADY BATCHED & SHUFFLED data\n",
        "# I think these low level Tensorflow models need to be fed a TF dataset\n",
        "\n",
        "# So feature column input is using dicts and tensors to navigate the input data\n",
        "\n",
        "# First step is to input the data & batch normalize\n",
        "# Input data\n",
        "features = tf.keras.layers.InputLayer()\n",
        "# batch normalize\n",
        "features = tf.keras.layers.BatchNormalization(features)\n",
        "# get size of batches\n",
        "batch_size = tf.shape(features[0])\n",
        "\n",
        "# Initializes decision-step dependent variables.\n",
        "\n",
        "# Dimensions of output, not sure what this will effect, as later is mapped to classify or regress\n",
        "output_aggregated = tf.zeros([batch_size, #output_dim])\n",
        "# instantiate masked features, this will be updated via feature selection process\n",
        "masked_features = features\n",
        "mask_values = tf.zeros([batch_size, #num_features])\n",
        "aggregated_mask_values = tf.zeros([batch_size, #num_features])\n",
        "complemantary_aggregated_mask_values = tf.ones([batch_size, #num_features])\n",
        "# entropy starting value\n",
        "total_entropy = 0\n",
        "\n",
        "# If we are training, the batch size is set\n",
        "if is_training:\n",
        "  v_b = #virtual_batch_size\n",
        "# if we are not training, the batch size is 1 for prediction(?)\n",
        "else:\n",
        "  v_b = 1\n",
        "\n",
        "tf.keras.layers.De\n",
        "\n",
        "# feed normalized data into feature transformer\n",
        "for ni in range(#num_decision_steps):\n",
        "\n",
        "  reuse_flag = (ni > 0)\n",
        "\n",
        " #Set to None to maintain a linear activation.  \n",
        "\n",
        "  transform_f1 = tf.keras.layers.Dense(\n",
        "      masked_features,\n",
        "      self.feature_dim * 2,\n",
        "      reuse=reuse_flag,\n",
        "      use_bias=False)\n",
        "  \n",
        "  transform_f1 = tf.layers.batch_normalization(\n",
        "      transform_f1,\n",
        "      training=is_training,\n",
        "      momentum=self.batch_momentum,\n",
        "      virtual_batch_size=v_b)\n",
        "      transform_f1 = glu(transform_f1, self.feature_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBjer9NoYlwW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLo_9W6nYlzZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUFcl5V4YmSp"
      },
      "source": [
        "# Kaggle Models\n",
        "\n",
        "https://www.kaggle.com/marcusgawronsky/tabnet-in-tensorflow-2-0/notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg-0gmFaYn_7"
      },
      "source": [
        "from typing import Optional, Union, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#GLU Block\n",
        "#The first component we are going to need to build is our GLUBlock which complises two fully \n",
        "#connected layers, two ghost batch normalization, our identity and sigmoid activation function \n",
        "#and multiplication operation. Here we use Tensorflow 2.0 custom layer subclassing to make this \n",
        "#layer easy to work with a reusable across the rest of our model. Here I have added a number of \n",
        "#type-hints for users to make working with this customer layer easy to follow and apply.\n",
        "\n",
        "class GLUBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None,\n",
        "                 virtual_batch_size: Optional[int] = 128, \n",
        "                 momentum: Optional[float] = 0.02):\n",
        "        super(GLUBlock, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "            \n",
        "        self.fc_outout = tf.keras.layers.Dense(self.units, \n",
        "                                               use_bias=False)\n",
        "        self.bn_outout = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                            momentum=self.momentum)\n",
        "        \n",
        "        self.fc_gate = tf.keras.layers.Dense(self.units, \n",
        "                                             use_bias=False)\n",
        "        self.bn_gate = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                          momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
        "        output = self.bn_outout(self.fc_outout(inputs), \n",
        "                                training=training)\n",
        "        gate = self.bn_gate(self.fc_gate(inputs), \n",
        "                            training=training)\n",
        "    \n",
        "        return output * tf.keras.activations.sigmoid(gate) # GLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYsBWh2pYoCZ"
      },
      "source": [
        "#Feature Transformer Block\n",
        "#Here we again use subclassing to define a layer to represent either the shared \n",
        "#or independent steps to 'Feature Transformer' in the diagram above. This block \n",
        "#comprises two GLU Blocks with a skip connection form the output of the first block \n",
        "#to the output of the second. Here I have had to add a flag to add a skip connection \n",
        "#over the first GLU Block, as the this is only present in the decision step dependent block.\n",
        "\n",
        "class FeatureTransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
        "                 momentum: Optional[float] = 0.02, skip=False):\n",
        "        super(FeatureTransformerBlock, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        self.skip = skip\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "        \n",
        "        self.initial = GLUBlock(units = self.units, \n",
        "                                virtual_batch_size=self.virtual_batch_size, \n",
        "                                momentum=self.momentum)\n",
        "        self.residual =  GLUBlock(units = self.units, \n",
        "                                  virtual_batch_size=self.virtual_batch_size, \n",
        "                                  momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None):\n",
        "        initial = self.initial(inputs, training=training)\n",
        "        \n",
        "        if self.skip == True:\n",
        "            initial += inputs\n",
        "\n",
        "        residual = self.residual(initial, training=training) # skip\n",
        "        \n",
        "        return (initial + residual) * np.sqrt(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBZYUbu3ZxAz"
      },
      "source": [
        "#Attention Block\n",
        "#This block is simple to implement and involves prior to the actual mask \n",
        "#operation, just a dense layer fed into a batch normalization layer, followed \n",
        "#by a sparsemax actication function. The major complication in this block is in \n",
        "#how to handle TabNet prior, used to encourage orthogonal feature selection \n",
        "#across decision steps. Here we just use it as an input to our layer and \n",
        "#reserve to handle the updates to our priors in our TabNet step layer.\n",
        "\n",
        "class AttentiveTransformer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int] = 128, \n",
        "                 momentum: Optional[float] = 0.02):\n",
        "        super(AttentiveTransformer, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "            \n",
        "        self.fc = tf.keras.layers.Dense(self.units, \n",
        "                                        use_bias=False)\n",
        "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                     momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs: Union[tf.Tensor, np.ndarray], priors: Optional[Union[tf.Tensor, np.ndarray]] = None, training: Optional[bool] = None) -> tf.Tensor:\n",
        "        feature = self.bn(self.fc(inputs), \n",
        "                          training=training)\n",
        "        if priors is None:\n",
        "            output = feature\n",
        "        else:\n",
        "            output = feature * priors\n",
        "        \n",
        "        return tfa.activations.sparsemax(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujUnddAdZxDY"
      },
      "source": [
        "#TabNetStep\n",
        "#In this TabNetStep Block I take a nunmber of design decision to make \n",
        "#implmentation and reusability simpler. At this layer we take as inputs our \n",
        "#batch normalized features, the output of our shared feature transformer, \n",
        "#and our priors of the current step and output the features embedding at our \n",
        "#split point, the masked feature to used in the shared feature transfomer black \n",
        "#of the next step and the mask used in our attention operation. This mask will \n",
        "#be important as we most though layers in ensuring new features are selected \n",
        "#across steps and providing local and global feature attributions for each \n",
        "#output. This block comprises our FeatureTransformerBlock and Attention \n",
        "#Transfomer block and starts to piece all our components together.\n",
        "\n",
        "class TabNetStep(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: Optional[int] = None, virtual_batch_size: Optional[int]=128, \n",
        "                 momentum: Optional[float] =0.02):\n",
        "        super(TabNetStep, self).__init__()\n",
        "        self.units = units\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.momentum = momentum\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):\n",
        "        if self.units is None:\n",
        "            self.units = input_shape[-1]\n",
        "        \n",
        "        self.unique = FeatureTransformerBlock(units = self.units, \n",
        "                                              virtual_batch_size=self.virtual_batch_size, \n",
        "                                              momentum=self.momentum,\n",
        "                                              skip=True)\n",
        "        self.attention = AttentiveTransformer(units = input_shape[-1], \n",
        "                                              virtual_batch_size=self.virtual_batch_size, \n",
        "                                              momentum=self.momentum)\n",
        "        \n",
        "    def call(self, inputs, shared, priors, training=None) -> Tuple[tf.Tensor]:  \n",
        "        split = self.unique(shared, training=training)\n",
        "        keys = self.attention(split, priors, training=training)\n",
        "        masked = keys * inputs\n",
        "        \n",
        "        return split, masked, keys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFN8ockfZxHQ"
      },
      "source": [
        "#TabNetEncoder\n",
        "#I opted to present the entire model architecture as a layer. This makes this \n",
        "#easier to work with between use cases, as we apply TabNet in unsupervised, \n",
        "#self-supervised and multiple supervised domains without having to rewrite \n",
        "#large tracts of code each time. You will see here, we accumulate our feature \n",
        "#embeddings at each decision step, update our priors and compute out entropy \n",
        "#loss used to limit how often features are reused across steps. This makes for \n",
        "#a complicated layer, but in many ways adds modularity which is very useful going forward.\n",
        "\n",
        "class TabNetEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, units: int =1, \n",
        "                 n_steps: int = 3, \n",
        "                 n_features: int = 8,\n",
        "                 outputs: int = 1, \n",
        "                 gamma: float = 1.3,\n",
        "                 epsilon: float = 1e-8, \n",
        "                 sparsity: float = 1e-5, \n",
        "                 virtual_batch_size: Optional[int]=128, \n",
        "                 momentum: Optional[float] =0.02):\n",
        "        super(TabNetEncoder, self).__init__()\n",
        "        \n",
        "        self.units = units\n",
        "        self.n_steps = n_steps\n",
        "        self.n_features = n_features\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.sparsity = sparsity\n",
        "        \n",
        "    def build(self, input_shape: tf.TensorShape):            \n",
        "        self.bn = tf.keras.layers.BatchNormalization(virtual_batch_size=self.virtual_batch_size, \n",
        "                                                     momentum=self.momentum)\n",
        "        self.shared_block = FeatureTransformerBlock(units = self.n_features, \n",
        "                                                    virtual_batch_size=self.virtual_batch_size, \n",
        "                                                    momentum=self.momentum)        \n",
        "        self.initial_step = TabNetStep(units = self.n_features, \n",
        "                                       virtual_batch_size=self.virtual_batch_size, \n",
        "                                       momentum=self.momentum)\n",
        "        self.steps = [TabNetStep(units = self.n_features, \n",
        "                                 virtual_batch_size=self.virtual_batch_size, \n",
        "                                 momentum=self.momentum) for _ in range(self.n_steps)]\n",
        "        self.final = tf.keras.layers.Dense(units = self.units, \n",
        "                                           use_bias=False)\n",
        "    \n",
        "\n",
        "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:        \n",
        "        entropy_loss = 0.\n",
        "        encoded = 0.\n",
        "        output = 0.\n",
        "        importance = 0.\n",
        "        prior = tf.reduce_mean(tf.ones_like(X), axis=0)\n",
        "        \n",
        "        B = prior * self.bn(X, training=training)\n",
        "        shared = self.shared_block(B, training=training)\n",
        "        _, masked, keys = self.initial_step(B, shared, prior, training=training)\n",
        "\n",
        "        for step in self.steps:\n",
        "            entropy_loss += tf.reduce_mean(tf.reduce_sum(-keys * tf.math.log(keys + self.epsilon), axis=-1)) / tf.cast(self.n_steps, tf.float32)\n",
        "            prior *= (self.gamma - tf.reduce_mean(keys, axis=0))\n",
        "            importance += keys\n",
        "            \n",
        "            shared = self.shared_block(masked, training=training)\n",
        "            split, masked, keys = step(B, shared, prior, training=training)\n",
        "            features = tf.keras.activations.relu(split)\n",
        "            \n",
        "            output += features\n",
        "            encoded += split\n",
        "            \n",
        "        self.add_loss(self.sparsity * entropy_loss)\n",
        "          \n",
        "        prediction = self.final(output)\n",
        "        return prediction, encoded, importance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xybs7BLaOv-"
      },
      "source": [
        "CATEGORICAL_COLUMNS = ['line_stat', 'serv_type', 'serv_code',\n",
        "                       'bandwidth', 'term_reas_code', 'term_reas_desc',\n",
        "                       'with_phone_service', 'current_mth_churn']\n",
        "NUMERIC_COLUMNS = ['contract_month', 'ce_expiry', 'secured_revenue', 'complaint_cnt']\n",
        "\n",
        "df = pd.read_csv('/content/bbs_cust_base_scfy_20200210.csv').assign(complaint_cnt = lambda df: pd.to_numeric(df.complaint_cnt, 'coerce'))\n",
        "\n",
        "df.loc[:, NUMERIC_COLUMNS] = df.loc[:, NUMERIC_COLUMNS].astype(np.float32).pipe(lambda df: df.fillna(df.mean())).pipe(lambda df: (df - df.mean())/df.std())\n",
        "\n",
        "df.loc[:, CATEGORICAL_COLUMNS] = df.loc[:, CATEGORICAL_COLUMNS].astype(str).applymap(str).fillna('')\n",
        "\n",
        "df = df.groupby('churn').apply(lambda df: df.sample(df.churn.value_counts().min()))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XONPkFEbJCO"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_labels(x: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Converts strings to unqiue ints for use in Pytorch Embedding\n",
        "    \"\"\"\n",
        "    labels, levels = pd.factorize(x)\n",
        "    return pd.Series(labels, name=x.name, index=x.index)\n",
        "\n",
        "X, E, y = (df\n",
        "           .loc[:, NUMERIC_COLUMNS]\n",
        "           .astype('float32')\n",
        "           .join(pd.get_dummies(df.loc[:, CATEGORICAL_COLUMNS])),\n",
        "           df\n",
        "           .loc[:, NUMERIC_COLUMNS]\n",
        "           .astype('float32')\n",
        "           .join(df.loc[:, CATEGORICAL_COLUMNS].apply(get_labels).add(1).astype('int32')),\n",
        "           df.churn == 'Y')\n",
        "\n",
        "X_train, X_valid, E_train, E_valid, y_train, y_valid = train_test_split(X.to_numpy(), E, y.to_numpy(), train_size=250000, test_size=250000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu6Rqa6Fbgjf"
      },
      "source": [
        "def get_feature(x: pd.DataFrame, dimension=1):\n",
        "    if x.dtype == np.float32:\n",
        "        return tf.feature_column.numeric_column(x.name)\n",
        "    else:\n",
        "        return tf.feature_column.embedding_column(\n",
        "        tf.feature_column.categorical_column_with_identity(x.name, num_buckets=x.max() + 1, default_value=0),\n",
        "        dimension=dimension)\n",
        "    \n",
        "def df_to_dataset(X: pd.DataFrame, y: pd.Series, shuffle=False, batch_size=50000):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(X.copy()), y.copy()))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(X))\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "columns = [get_feature(f) for k, f in E_train.iteritems()]\n",
        "feature_column = tf.keras.layers.DenseFeatures(columns, trainable=True)\n",
        "\n",
        "train, valid = df_to_dataset(E_train, y_train), df_to_dataset(E_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8uc5ve7cPPI"
      },
      "source": [
        "#The first application we will be looking at is in supervised learning this \n",
        "#is a primary aim of TabNet so is one we will explore. Here I tried to trick \n",
        "#to a number of hyperparameter defaults found in other implementation, exploring \n",
        "#only a smaller feature vector size for the purpose of visualization later on. \n",
        "#In my experiment this hampers the formance of the model greatly but, in my \n",
        "#implementation, reduces greatly the overall footprint of the model given \n",
        "#the use of weight sharing across the steps.\n",
        "\n",
        "#Here I use Tensorflow 2's model subclassing approach to make \n",
        "#explainations and feature visualization easier later on. For \n",
        "#production use, the subclassing API does have some limitation in \n",
        "#how model can be serialized and unserialized- some of which have \n",
        "#been adressed in Tensorflow 2.2 and 2.3 releases.\n",
        "\n",
        "class TabNetClassifier(tf.keras.Model):\n",
        "    def __init__(self, outputs: int = 1, \n",
        "                 n_steps: int = 3, \n",
        "                 n_features: int = 8,\n",
        "                 gamma: float = 1.3, \n",
        "                 epsilon: float = 1e-8, \n",
        "                 sparsity: float = 1e-5, \n",
        "                 feature_column: Optional[tf.keras.layers.DenseFeatures] = None, \n",
        "                 pretrained_encoder: Optional[tf.keras.layers.Layer] = None,\n",
        "                 virtual_batch_size: Optional[int] = 128, \n",
        "                 momentum: Optional[float] = 0.02):\n",
        "        super(TabNetClassifier, self).__init__()\n",
        "        \n",
        "        self.outputs = outputs\n",
        "        self.n_steps = n_steps\n",
        "        self.n_features = n_features\n",
        "        self.feature_column = feature_column\n",
        "        self.pretrained_encoder = pretrained_encoder\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.sparsity = sparsity\n",
        "        \n",
        "        if feature_column is None:\n",
        "            self.feature = tf.keras.layers.Lambda(identity)\n",
        "        else:\n",
        "            self.feature = feature_column\n",
        "            \n",
        "        if pretrained_encoder is None:\n",
        "            self.encoder = TabNetEncoder(units=outputs, \n",
        "                                        n_steps=n_steps, \n",
        "                                        n_features = n_features,\n",
        "                                        outputs=outputs, \n",
        "                                        gamma=gamma, \n",
        "                                        epsilon=epsilon, \n",
        "                                        sparsity=sparsity,\n",
        "                                        virtual_batch_size=self.virtual_batch_size, \n",
        "                                        momentum=momentum)\n",
        "        else:\n",
        "            self.encoder = pretrained_encoder\n",
        "\n",
        "    def forward(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> Tuple[tf.Tensor]:\n",
        "        X = self.feature(X)\n",
        "        output, encoded, importance = self.encoder(X)\n",
        "          \n",
        "        prediction = tf.keras.activations.sigmoid(output)\n",
        "        return prediction, encoded, importance\n",
        "    \n",
        "    def call(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
        "        prediction, _, _ = self.forward(X)\n",
        "        return prediction\n",
        "    \n",
        "    def transform(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
        "        _, encoded, _ = self.forward(X)\n",
        "        return encoded\n",
        "    \n",
        "    def explain(self, X: Union[tf.Tensor, np.ndarray], training: Optional[bool] = None) -> tf.Tensor:\n",
        "        _, _, importance = self.forward(X)\n",
        "        return importance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6owlzuc8fe"
      },
      "source": [
        "m = TabNetClassifier(outputs=1, n_steps=3, n_features = 2, feature_column=feature_column, virtual_batch_size=250)\n",
        "m.compile(tf.keras.optimizers.Adam(learning_rate=0.025), tf.keras.losses.binary_crossentropy)\n",
        "m.fit(train, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl1gjv3mfZnO",
        "outputId": "c272cca5-5003-42b8-a441-6b5c40c4d0ec"
      },
      "source": [
        "tf_tabnet_y_pred = m.predict(train)\n",
        "\n",
        "accuracy_score(y_train, tf_tabnet_y_pred > 0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.875036"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSy2L8fhfhAd",
        "outputId": "83f4dd78-badb-45f2-dd4d-f766693d986a"
      },
      "source": [
        "tf_tabnet_y_pred = m.predict(valid)\n",
        "\n",
        "accuracy_score(y_valid, tf_tabnet_y_pred > 0.5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.876124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yl3KI4OmASg"
      },
      "source": [
        "# TabNet in TF 2.0 from Ostamand\n",
        "\n",
        "https://github.com/ostamand/tensorflow-tabnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FikEs4jRgn2-"
      },
      "source": [
        "## Notes:\n",
        "\n",
        "The FeatureBlock function is nested within the FeatureTransformer\n",
        "\n",
        "Need to understand how to fit this model, which requires understanding of each sub process block. If I can understand what is happening, then I can make my own NN with attention blocks, transformers, encoders, etc.\n",
        "\n",
        "Notes 12/26\n",
        "\n",
        "Try to write your own model using feature transformers & attention blocks using classes\n",
        "\n",
        "(1) understand how to implement a model using classes\n",
        "\n",
        "(2) understand how to assign tensorflow names using ->\n",
        "\n",
        "(3) understand the building of the model with the loop & blocks\n",
        "\n",
        "Understanding\n",
        "\n",
        "TabNet uses soft feature selection via an embedding matrix\n",
        "\n",
        "\"[TabNet] embeds soft feature selection with controllable\n",
        "sparsity via sequential attention.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTDfXTqyPhwz"
      },
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow_addons.activations import sparsemax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uDomGnyOnrL"
      },
      "source": [
        "class FeatureBlock(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_dim: int,\n",
        "        apply_glu: bool = True,\n",
        "        bn_momentum: float = 0.9,\n",
        "        bn_virtual_divider: int = 32,\n",
        "        fc: tf.keras.layers.Layer = None,\n",
        "        epsilon: float = 1e-5,\n",
        "    ):\n",
        "        super(FeatureBlock, self).__init__()\n",
        "        self.apply_gpu = apply_glu\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        units = feature_dim * 2 if apply_glu else feature_dim\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(units, use_bias=False) if fc is None else fc\n",
        "        self.bn = GhostBatchNormalization(virtual_divider=bn_virtual_divider, momentum=bn_momentum)\n",
        "\n",
        "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
        "        x = self.fc(x)\n",
        "        x = self.bn(x, training=training, alpha=alpha)\n",
        "        if self.apply_gpu:\n",
        "            return glu(x, self.feature_dim)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaCxQQFiOnty"
      },
      "source": [
        "class AttentiveTransformer(tf.keras.Model):\n",
        "    def __init__(self, feature_dim: int, bn_momentum: float, bn_virtual_divider: int):\n",
        "        super(AttentiveTransformer, self).__init__()\n",
        "        self.block = FeatureBlock(\n",
        "            feature_dim,\n",
        "            bn_momentum=bn_momentum,\n",
        "            bn_virtual_divider=bn_virtual_divider,\n",
        "            apply_glu=False,\n",
        "        )\n",
        "\n",
        "    def call(self, x, prior_scales, training=None, alpha: float = 0.0):\n",
        "        x = self.block(x, training=training, alpha=alpha)\n",
        "        return sparsemax(x * prior_scales)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVR3OB95P1U3"
      },
      "source": [
        "def glu(x, n_units=None):\n",
        "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
        "    return x[:, :n_units] * tf.nn.sigmoid(x[:, n_units:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe-Ngs3PPaei"
      },
      "source": [
        "class FeatureTransformer(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_dim: int,\n",
        "        fcs: List[tf.keras.layers.Layer] = [],\n",
        "        n_total: int = 4,\n",
        "        n_shared: int = 2,\n",
        "        bn_momentum: float = 0.9,\n",
        "        bn_virtual_divider: int = 1,\n",
        "    ):\n",
        "        super(FeatureTransformer, self).__init__()\n",
        "        self.n_total, self.n_shared = n_total, n_shared\n",
        "\n",
        "        kargs = {\n",
        "            \"feature_dim\": feature_dim,\n",
        "            \"bn_momentum\": bn_momentum,\n",
        "            \"bn_virtual_divider\": bn_virtual_divider,\n",
        "        }\n",
        "\n",
        "        # build blocks\n",
        "        self.blocks: List[FeatureBlock] = []\n",
        "        for n in range(n_total):\n",
        "            # some shared blocks\n",
        "            if fcs and n < len(fcs):\n",
        "                self.blocks.append(FeatureBlock(**kargs, fc=fcs[n]))\n",
        "            # build new blocks\n",
        "            else:\n",
        "                self.blocks.append(FeatureBlock(**kargs))\n",
        "\n",
        "    def call(self, x: tf.Tensor, training: bool = None, alpha: float = 0.0) -> tf.Tensor:\n",
        "      \n",
        "        x = self.blocks[0](x, training=training, alpha=alpha)\n",
        "        for n in range(1, self.n_total):\n",
        "            x = x * tf.sqrt(0.5) + self.blocks[n](x, training=training, alpha=alpha)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def shared_fcs(self):\n",
        "        return [self.blocks[i].fc for i in range(self.n_shared)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv8_dnzUQBZF"
      },
      "source": [
        "class GhostBatchNormalization(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self, virtual_divider: int = 1, momentum: float = 0.9, epsilon: float = 1e-5\n",
        "    ):\n",
        "        super(GhostBatchNormalization, self).__init__()\n",
        "        self.virtual_divider = virtual_divider\n",
        "        self.bn = BatchNormInferenceWeighting(momentum=momentum)\n",
        "\n",
        "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
        "        if training:\n",
        "            chunks = tf.split(x, self.virtual_divider)\n",
        "            x = [self.bn(x, training=True) for x in chunks]\n",
        "            return tf.concat(x, 0)\n",
        "        return self.bn(x, training=False, alpha=alpha)\n",
        "\n",
        "    @property\n",
        "    def moving_mean(self):\n",
        "        return self.bn.moving_mean\n",
        "\n",
        "    @property\n",
        "    def moving_variance(self):\n",
        "        return self.bn.moving_variance\n",
        "\n",
        "\n",
        "class BatchNormInferenceWeighting(tf.keras.layers.Layer):\n",
        "    def __init__(self, momentum: float = 0.9, epsilon: float = None):\n",
        "        super(BatchNormInferenceWeighting, self).__init__()\n",
        "        self.momentum = momentum\n",
        "        self.epsilon = tf.keras.backend.epsilon() if epsilon is None else epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "\n",
        "        self.gamma = tf.Variable(\n",
        "            initial_value=tf.ones((channels,), tf.float32), trainable=True,)\n",
        "        \n",
        "        self.beta = tf.Variable(\n",
        "            initial_value=tf.zeros((channels,), tf.float32), trainable=True,)\n",
        "\n",
        "        self.moving_mean = tf.Variable(\n",
        "            initial_value=tf.zeros((channels,), tf.float32), trainable=False,)\n",
        "        \n",
        "        self.moving_mean_of_squares = tf.Variable(\n",
        "            initial_value=tf.zeros((channels,), tf.float32), trainable=False,)\n",
        "\n",
        "    def __update_moving(self, var, value):\n",
        "        var.assign(var * self.momentum + (1 - self.momentum) * value)\n",
        "\n",
        "    def __apply_normalization(self, x, mean, variance):\n",
        "        return self.gamma * (x - mean) / tf.sqrt(variance + self.epsilon) + self.beta\n",
        "\n",
        "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
        "        mean = tf.reduce_mean(x, axis=0)\n",
        "        mean_of_squares = tf.reduce_mean(tf.pow(x, 2), axis=0)\n",
        "\n",
        "        if training:\n",
        "            # update moving stats\n",
        "            self.__update_moving(self.moving_mean, mean)\n",
        "            self.__update_moving(self.moving_mean_of_squares, mean_of_squares)\n",
        "\n",
        "            variance = mean_of_squares - tf.pow(mean, 2)\n",
        "            x = self.__apply_normalization(x, mean, variance)\n",
        "        else:\n",
        "            mean = alpha * mean + (1 - alpha) * self.moving_mean\n",
        "            variance = (alpha * mean_of_squares + (1 - alpha) * self.moving_mean_of_squares) - tf.pow(mean, 2)\n",
        "            x = self.__apply_normalization(x, mean, variance)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a6m-7MAPahc"
      },
      "source": [
        "# Model\n",
        "class TabNet(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_features: int,\n",
        "        feature_dim: int,\n",
        "        output_dim: int,\n",
        "        feature_columns: List = None,\n",
        "        n_step: int = 1,\n",
        "        n_total: int = 4,\n",
        "        n_shared: int = 2,\n",
        "        relaxation_factor: float = 1.5,\n",
        "        bn_epsilon: float = 1e-5,\n",
        "        bn_momentum: float = 0.7,\n",
        "        bn_virtual_divider: int = 1,\n",
        "    ):\n",
        "        \"\"\"TabNet\n",
        "        Will output a vector of size output_dim.\n",
        "        Args:\n",
        "            num_features (int): Number of features.\n",
        "            feature_dim (int): Embedding feature dimention to use.\n",
        "            output_dim (int): Output dimension.\n",
        "            feature_columns (List, optional): If defined will add a DenseFeatures layer first. Defaults to None.\n",
        "            n_step (int, optional): Total number of steps. Defaults to 1.\n",
        "            n_total (int, optional): Total number of feature transformer blocks. Defaults to 4.\n",
        "            n_shared (int, optional): Number of shared feature transformer blocks. Defaults to 2.\n",
        "            relaxation_factor (float, optional): >1 will allow features to be used more than once. Defaults to 1.5.\n",
        "            bn_epsilon (float, optional): Batch normalization, epsilon. Defaults to 1e-5.\n",
        "            bn_momentum (float, optional): Batch normalization, momentum. Defaults to 0.7.\n",
        "            bn_virtual_divider (int, optional): Batch normalization. Full batch will be divided by this.\n",
        "        \"\"\"\n",
        "        super(TabNet, self).__init__()\n",
        "        self.output_dim, self.num_features = output_dim, num_features\n",
        "        self.n_step, self.relaxation_factor = n_step, relaxation_factor\n",
        "        self.feature_columns = feature_columns\n",
        "\n",
        "        if feature_columns is not None:\n",
        "            self.input_features = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "        self.bn = tf.keras.layers.BatchNormalization(momentum=bn_momentum, epsilon=bn_epsilon)\n",
        "\n",
        "        kargs = {\n",
        "            \"feature_dim\": feature_dim + output_dim,\n",
        "            \"n_total\": n_total,\n",
        "            \"n_shared\": n_shared,\n",
        "            \"bn_momentum\": bn_momentum,\n",
        "            \"bn_virtual_divider\": bn_virtual_divider,\n",
        "        }\n",
        "\n",
        "        # first feature transformer block is built first to get the shared blocks\n",
        "        self.feature_transforms: List[FeatureTransformer] = [FeatureTransformer(**kargs)]\n",
        "        self.attentive_transforms: List[AttentiveTransformer] = []\n",
        "\n",
        "        for i in range(n_step):\n",
        "            self.feature_transforms.append(\n",
        "                FeatureTransformer(**kargs, fcs=self.feature_transforms[0].shared_fcs)\n",
        "            )\n",
        "            self.attentive_transforms.append(\n",
        "                AttentiveTransformer(num_features, bn_momentum, bn_virtual_divider)\n",
        "            )\n",
        "\n",
        "    def call(self, features: tf.Tensor, training: bool = None, alpha: float = 0.0) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        if self.feature_columns is not None:\n",
        "            features = self.input_features(features)\n",
        "\n",
        "        bs = tf.shape(features)[0]\n",
        "        out_agg = tf.zeros((bs, self.output_dim))\n",
        "        prior_scales = tf.ones((bs, self.num_features))\n",
        "        masks = []\n",
        "\n",
        "        features = self.bn(features, training=training)\n",
        "        masked_features = features\n",
        "\n",
        "        total_entropy = 0.0\n",
        "\n",
        "        for step_i in range(self.n_step + 1):\n",
        "\n",
        "            x = self.feature_transforms[step_i](masked_features, training=training, alpha=alpha)\n",
        "\n",
        "            if step_i > 0:\n",
        "                out = tf.keras.activations.relu(x[:, : self.output_dim])\n",
        "                out_agg += out\n",
        "\n",
        "            # no need to build the features mask for the last step\n",
        "            if step_i < self.n_step:\n",
        "                x_for_mask = x[:, self.output_dim :]\n",
        "\n",
        "                mask_values = self.attentive_transforms[step_i](x_for_mask, prior_scales, training=training, alpha=alpha)\n",
        "\n",
        "                # relaxation factor of 1 forces the feature to be only used once.\n",
        "                prior_scales *= self.relaxation_factor - mask_values\n",
        "\n",
        "                masked_features = tf.multiply(mask_values, features)\n",
        "\n",
        "                # entropy is used to penalize the amount of sparsity in feature selection\n",
        "                total_entropy = tf.reduce_mean(\n",
        "                    tf.reduce_sum(\n",
        "                        tf.multiply(mask_values, \n",
        "                                    tf.math.log(mask_values + 1e-15)),\n",
        "                        axis=1,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                masks.append(tf.expand_dims(tf.expand_dims(mask_values, 0), 3))\n",
        "\n",
        "        loss = total_entropy / self.n_step\n",
        "\n",
        "        return out_agg, loss, masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5kaKoXPbR1B"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ega-gWpMchK5"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYPakl8DPakq"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "def prepare_dataset(\n",
        "    ds: tf.data.Dataset,\n",
        "    batch_size: int,\n",
        "    shuffle: bool = False,\n",
        "    drop_remainder: bool = False,\n",
        "):\n",
        "    size_of_dataset = ds.reduce(0, lambda x, _: x + 1).numpy()\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=size_of_dataset, seed=SEED)\n",
        "    ds: tf.data.Dataset = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
        "\n",
        "    @tf.function\n",
        "    def prepare_data(features):\n",
        "        image = tf.cast(features[\"image\"], tf.float32)\n",
        "        bs = tf.shape(image)[0]\n",
        "        image = tf.reshape(image / 255.0, (bs, -1))\n",
        "        return image, features[\"label\"]\n",
        "\n",
        "    autotune = tf.data.experimental.AUTOTUNE\n",
        "    ds = ds.map(prepare_data, num_parallel_calls=autotune).prefetch(autotune)\n",
        "    return ds\n",
        "\n",
        "    # first 80% for train. remaining 20% for val & test dataset for final eval.\n",
        "ds_tr, ds_val, ds_test = tfds.load(\n",
        "        name=\"mnist\",\n",
        "        split=[\"train[:80%]\", \"train[-20%:]\", \"test\"],\n",
        "        data_dir=\"mnist\",\n",
        "        shuffle_files=False,\n",
        "    )\n",
        "\n",
        "batch_size = 32\n",
        "SEED = 42\n",
        "ds_tr = prepare_dataset(ds_tr, batch_size, shuffle=True, drop_remainder=True)\n",
        "ds_val = prepare_dataset(ds_val, batch_size, shuffle=False, drop_remainder=False)\n",
        "ds_test = prepare_dataset(ds_test, batch_size, shuffle=False, drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXvcYxNwcjHH"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf_zyYoMck3I"
      },
      "source": [
        "DEFAULTS = {\"num_features\": 784, \"n_classes\": 10, \"min_learning_rate\": 1e-6} \n",
        "\n",
        "model = TabNetClassifier(\n",
        "        num_features=DEFAULTS[\"num_features\"],\n",
        "        feature_dim= 32,\n",
        "        output_dim= 32,\n",
        "        n_classes=DEFAULTS[\"n_classes\"],\n",
        "        n_step= 4,\n",
        "        relaxation_factor=1.5,\n",
        "        sparsity_coefficient=0.0001,\n",
        "        bn_momentum=0.7,\n",
        "        bn_virtual_divider=1,  # let's not use Ghost Batch Normalization. batch sizes are too small\n",
        "        dp=0.0)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.02,\n",
        "        clipnorm=2,\n",
        "    )\n",
        "\n",
        "lossf = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(\n",
        "        optimizer,\n",
        "        loss=lossf,\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0BR_O60ck5x"
      },
      "source": [
        "model.fit(ds_tr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2CA6IGl8tr4"
      },
      "source": [
        "def glu(act, n_units):\r\n",
        "  \"\"\"Generalized linear unit nonlinear activation.\"\"\"\r\n",
        "  return act[:, :n_units] * tf.nn.sigmoid(act[:, n_units:])\r\n",
        "\r\n",
        "\r\n",
        "class Transformer(layers.Layer):\r\n",
        "  def __init__(self,feature_dim):\r\n",
        "\r\n",
        "    super(Transformer,self).__init__()\r\n",
        "\r\n",
        "    units = feature_dim * 2\r\n",
        "    self.dense = layers.Dense(units,use_bias=False)\r\n",
        "    self.bn = layers.BatchNormalization()\r\n",
        "\r\n",
        "  def call(self,x,training=False):\r\n",
        "    x = self.dense(x)\r\n",
        "    x = self.bn(x, training = training)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "class MyModel(keras.Model):\r\n",
        "  def __init__(self,feature_dim):\r\n",
        "\r\n",
        "    super(MyModel,self).__init__()\r\n",
        "    self.feature_dim = feature_dim\r\n",
        "    \r\n",
        "    self.one = Transformer(self.feature_dim)\r\n",
        "    self.two = Transformer(self.feature_dim)\r\n",
        "    self.three = Transformer(self.feature_dim)\r\n",
        "    self.four = Transformer(self.feature_dim)\r\n",
        "    self.out = layers.Dense(1)\r\n",
        "\r\n",
        "# I think if I were to use logic it would be in the call\r\n",
        "  def call(self,input_tensor):\r\n",
        "    x = self.one(input_tensor)\r\n",
        "    t1=glu(x,self.feature_dim)\r\n",
        "    x = glu(x,self.feature_dim)  \r\n",
        "   \r\n",
        "    x = self.two(x)\r\n",
        "    t2 = glu(x,self.feature_dim)\r\n",
        "    x = (glu(x,self.feature_dim)+t1)*np.sqrt(0.5)\r\n",
        "    # I don't understand what is coming out of these models so it is hard to tell what is going on\r\n",
        "    # doesn't make well for understanding\r\n",
        "    \r\n",
        "    x = self.three(x)\r\n",
        "    t3 = glu(x,self.feature_dim)\r\n",
        "    x = (glu(x,self.feature_dim)+t2)*np.sqrt(0.5)\r\n",
        "\r\n",
        "    x = self.four(x)\r\n",
        "    x = (glu(x,self.feature_dim)+t3)*np.sqrt(0.5)\r\n",
        "\r\n",
        "    x = tf.nn.relu(x)\r\n",
        "    return self.out(x)\r\n",
        "\r\n",
        "# Not sure how to do the sequential attention, or the masked features\r\n",
        "# I think I have the layers, probably not though\r\n",
        "\r\n",
        "model = MyModel(feature_dim = X.shape[1])\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    loss = 'mse',\r\n",
        "    metrics=[tf.keras.metrics.MeanSquaredError()]\r\n",
        ")\r\n",
        "\r\n",
        "model.fit(X,y,epochs=20,batch_size=40)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIPp7DdE7Xgd"
      },
      "source": [
        "# Object Oriented Tensorflow Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMk7_oAXzG3F"
      },
      "source": [
        "\r\n",
        "## Subclassing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEV-urYc7fd7"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD9bO0U67fhY"
      },
      "source": [
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1,28,28,1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1,28,28,1).astype(\"float32\") / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQe1muY5-yBk"
      },
      "source": [
        "class CNNBlock(layers.Layer):\n",
        "  def __init__(self,out_channels,kernel_size=3):\n",
        "    super(CNNBlock,self).__init__()\n",
        "    self.conv = layers.Conv2D(out_channels, kernel_size, padding='same')\n",
        "    self.bn = layers.BatchNormalization()\n",
        "\n",
        "  def call(self,input_tensor, training=False):\n",
        "    x = self.conv(input_tensor)\n",
        "    x = self.bn(x, training=training)\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "class ResBlock(layers.Layer):\n",
        "  def __init__(self,channels):\n",
        "    super(ResBlock,self).__init__()\n",
        "    self.cnn1 = CNNBlock(channels[0])\n",
        "    self.cnn2 = CNNBlock(channels[1])\n",
        "    self.cnn3 = CNNBlock(channels[2])\n",
        "    self.pooling = layers.MaxPooling2D()\n",
        "    self.identity_mapping = layers.Conv2D(channels[1],1, padding='same')\n",
        "\n",
        "  def call(self,input_tensor,training=False):\n",
        "    x = self.cnn1(input_tensor,training=training)\n",
        "    x = self.cnn2(x,training=training)\n",
        "    x = self.cnn3(\n",
        "        x + self.identity_mapping(input_tensor), training=training\n",
        "    )\n",
        "    return self.pooling(x)\n",
        "\n",
        "\n",
        "class ResNet_Like(keras.Model):\n",
        "  def __init__(self,num_classes=10):\n",
        "    super(ResNet_Like,self).__init__()\n",
        "    self.block1 = ResBlock([32,32,64])\n",
        "    self.block2 = ResBlock([128,128,265])\n",
        "    self.block3 = ResBlock([128,265,512])\n",
        "    self.pool = layers.GlobalAveragePooling2D()\n",
        "    self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "  def call(self,input_tensor,training=False):\n",
        "    x = self.block1(input_tensor,training=training)\n",
        "    x = self.block2(x,training=training)\n",
        "    x = self.block3(x,training=training)\n",
        "    x = self.pool(x)\n",
        "    return self.classifier(x)\n",
        "\n",
        "  def model(self):\n",
        "    x = keras.Input(shape=(28,28,1))\n",
        "    return keras.Model(inputs=[x], outputs=self.call(x))\n",
        "\n",
        "model = ResNet_Like(num_classes=10)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49VxiTI-Af66"
      },
      "source": [
        "model.fit(x_train,y_train,batch_size=64,epochs=1)\n",
        "print(model.model().summary())\n",
        "model.evaluate(x_test,y_test,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZkQOdn6CAlw",
        "outputId": "932e295f-8a9b-49d7-b952-78819e4e2c35"
      },
      "source": [
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1,28*28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1,28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "class Dense(layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(Dense,self).__init__()\n",
        "    self.units = units\n",
        "   \n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.w = self.add_weight(\n",
        "        name = 'w',\n",
        "        shape=(input_shape[-1],self.units),\n",
        "        initializer='random_normal',\n",
        "        trainable=True,\n",
        "    )\n",
        "    self.b = self.add_weight(\n",
        "        name='b', shape=(self.units,), initializer = 'zeros', trainable=True,\n",
        "    )\n",
        "\n",
        "  def call(self,inputs):\n",
        "    return tf.matmul(inputs,self.w) + self.b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyRelu(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(MyRelu,self).__init__()\n",
        "\n",
        "  def call(self,x):\n",
        "    return tf.math.maximum(x,0)\n",
        "\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "  def __init__(self,num_classes=10):\n",
        "    super(MyModel,self).__init__()\n",
        "    self.dense1 = Dense(64)\n",
        "    self.dense2 = Dense(num_classes)\n",
        "    #self.dense1 = layers.Dense(64)\n",
        "    #self.dense2 = layers.Dense(num_classes)\n",
        "    self.relu = MyRelu()\n",
        "\n",
        "  def call(self,input_tensor):\n",
        "    x = self.relu(self.dense1(input_tensor))\n",
        "    return self.dense2(x)\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(x_train,y_train,batch_size=32,epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5910 - accuracy: 0.8403\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa7a4dad438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr0G2oqGzmlF"
      },
      "source": [
        "## Understanding Object Orientation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyvR-vbkhz75"
      },
      "source": [
        "# with class statement define what you are building\r\n",
        "# I know thus far about Layers.layer & keras.Model\r\n",
        "class CustLay(Layers.layer):\r\n",
        "  # start with defining whats init\r\n",
        "  def __init__(self,units):\r\n",
        "    # this super commant is a little confusing, very object-y\r\n",
        "    super(CustLay,self).__init__()\r\n",
        "\r\n",
        "    # this layer has a fully connected dense layer & a batch norm layer\r\n",
        "    self.dense = layers.Dense(units)\r\n",
        "    self.bn = layers.BatchNormalization()\r\n",
        "\r\n",
        "  # next define the call\r\n",
        "  # the call uses the self attributes defined above & applies them to the passed through x\r\n",
        "  def call(self, x):\r\n",
        "    x = self.dense(x)\r\n",
        "    x = self.bn(x)\r\n",
        "    return x\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTHdKA9KJyid"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wj-D5hV-t76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049dec54-7f77-4b9e-bc3c-372481d22852"
      },
      "source": [
        "class Flex(layers.Layer):\r\n",
        "  def __init__(self,out_features,**kwargs):\r\n",
        "    super(Flex,self).__init__(**kwargs)\r\n",
        "    self.out_features = out_features\r\n",
        "\r\n",
        "  # this build step allows for flexible input\r\n",
        "  def build(self,input_shape):\r\n",
        "    self.w = tf.Variable(\r\n",
        "        tf.random.normal([input_shape[-1],self.out_features]), name='w')\r\n",
        "    self.b = tf.Variable(tf.zeros([self.out_features]), name = 'b')\r\n",
        "\r\n",
        "  def call(self,inputs):\r\n",
        "    # matrix multiplication of random variable + zeros\r\n",
        "    return tf.matmul(inputs,self.w) + self.b\r\n",
        "\r\n",
        "flex_dense = Flex(out_features=3)\r\n",
        "\r\n",
        "flex_dense.variables\r\n",
        "\r\n",
        "# Call it, with predictably random results\r\n",
        "print(\"Model results:\", flex_dense(tf.constant([[2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])))\r\n",
        "flex_dense.variables"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model results: tf.Tensor(\n",
            "[[-5.2164702  2.7636783  2.7398803]\n",
            " [-7.8247046  4.1455173  4.1098204]], shape=(2, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'flex_12/w:0' shape=(3, 3) dtype=float32, numpy=\n",
              " array([[-1.41189   ,  0.39761004,  1.5689397 ],\n",
              "        [-0.8508605 , -0.85104465,  0.0049968 ],\n",
              "        [-0.34548447,  1.8352737 , -0.20399626]], dtype=float32)>,\n",
              " <tf.Variable 'flex_12/b:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQR6-nkTmG4o",
        "outputId": "8a402f24-752f-48e7-c8de-e639a577f22b"
      },
      "source": [
        "try:\r\n",
        "  print(\"Model results:\", flex_dense(tf.constant([[2.0, 2.0, 2.0, 2.0]])))\r\n",
        "except tf.errors.InvalidArgumentError as e:\r\n",
        "  print(\"Failed:\", e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Failed: Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgbC6ji-mef_",
        "outputId": "478bf3da-affb-4d82-8c17-5e85405dbbc6"
      },
      "source": [
        "class MySequentialModel(tf.keras.Model):\r\n",
        "  def __init__(self, name=None, **kwargs):\r\n",
        "    super().__init__(**kwargs)\r\n",
        "\r\n",
        "    self.dense_1 = flex_dense\r\n",
        "    self.dense_2 = Flex(out_features=2)\r\n",
        "  def call(self, x):\r\n",
        "    x = self.dense_1(x)\r\n",
        "    return self.dense_2(x)\r\n",
        "\r\n",
        "# You have made a Keras model!\r\n",
        "my_sequential_model = MySequentialModel(name=\"the_model\")\r\n",
        "\r\n",
        "# Call it on a tensor, with random results\r\n",
        "print(\"Model results:\", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model results: tf.Tensor([[2.144764 3.754323]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFngv0BrzLBk"
      },
      "source": [
        "## Mixing with Functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_l5n72KqUjX"
      },
      "source": [
        "# a look at mixing with functional API\r\n",
        "inputs = tf.keras.Input(shape=[3,])\r\n",
        "\r\n",
        "x = Flex(3)(inputs)\r\n",
        "x = Flex(2)(x)\r\n",
        "\r\n",
        "my_functional_model = tf.keras.Model(inputs=inputs,outputs=x)\r\n",
        "\r\n",
        "my_functional_model.summary()\r\n",
        "\r\n",
        "my_functional_model(tf.constant([[2.0, 2.0, 2.0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzwGsS13zSmc"
      },
      "source": [
        "## What is sharing & decision steps?\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKATcuULAJEG"
      },
      "source": [
        "# I can build a simple model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUZUuGQV6wvk"
      },
      "source": [
        "## Basics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o33eXZE0qUmS"
      },
      "source": [
        "# Dense Layer\r\n",
        "class Dense(layers.Layer):\r\n",
        "  def __init__(self,units,input_dim):\r\n",
        "    super(Dense,self).__init__()\r\n",
        "    self.dense = layers.Dense(units)\r\n",
        "\r\n",
        "  def call(self, x):\r\n",
        "    x = self.dense(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "# BN Layer\r\n",
        "class Batch(layers.Layer):\r\n",
        "  def __init__(self):\r\n",
        "    super(Batch,self).__init__()\r\n",
        "    self.bn = layers.BatchNormalization()\r\n",
        "  def call(self,x):\r\n",
        "    return self.bn(x)\r\n",
        "\r\n",
        "# Ok so this model works\r\n",
        "# my model has two fully connected layers\r\n",
        "# the output dim is done in the __init__\r\n",
        "\r\n",
        "class ModMod(keras.Model):\r\n",
        "  def __init__(self,name=None,output_dim=10):\r\n",
        "    super(ModMod,self).__init__()\r\n",
        "    self.dense1 = layers.Dense(20)\r\n",
        "    self.batch = layers.BatchNormalization()\r\n",
        "    self.Batch = Batch()\r\n",
        "    self.dense2 = layers.Dense(output_dim)\r\n",
        "\r\n",
        "\r\n",
        "  # ok I got it to work\r\n",
        "  # was breaking bc of the input/output of batch normalization\r\n",
        "  # I did not specify the input, but then I did & it works\r\n",
        "  def call(self,input):\r\n",
        "    x = tf.nn.relu(self.dense1(input))\r\n",
        "    x = self.Batch(x)\r\n",
        "    return self.dense2(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1owa5uDUFo5r"
      },
      "source": [
        "# need to mix this with the functional API\r\n",
        "# can you loop in the FAPI?\r\n",
        "# if so, I think you could recreate the tabnet\r\n",
        "\r\n",
        "# focus on sharing & decision step next\r\n",
        "# create a transformer & see what that does/is"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zShEf6h1v1SX"
      },
      "source": [
        "path = \"https://raw.githubusercontent.com/shrikant-temburwar/Wine-Quality-Dataset/master/winequality-white.csv\"\n",
        "df = pd.read_csv(path,delimiter=\";\")\n",
        "\n",
        "X = df.loc[:,df.columns != 'quality']\n",
        "y = df['quality']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AosmlB-mEnT2",
        "outputId": "666cc668-913c-4ed6-e2bd-fc421f1f4d7a"
      },
      "source": [
        "model = ModMod()\r\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\r\n",
        "\r\n",
        "model.fit(x_train,y_train,epochs=10)\r\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 1s 1ms/step - loss: 34.5594\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 24.9945\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 10.5722\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 2.2974\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 0.8776\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 0.6543\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 0.6297\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 0.6305\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 0.5832\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 0s 1ms/step - loss: 0.5976\n",
            "39/39 [==============================] - 0s 796us/step - loss: 0.6614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6614018678665161"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}